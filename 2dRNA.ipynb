{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants & Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "BULK_PATH = \"input/2dRNA/group1/bulk_RawCounts.tsv\"\n",
    "SC_DIR_PATH = \"input/2dRNA/group1/\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "np.set_printoptions(linewidth=120)\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading\n",
    "Load necessary files to DataFrames, see info/stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Matrix (Tissue GEPs) Sample:\n",
      "\n",
      "             gene_id  gene_symbol  CANUCK1057-BAL-LB3B  CANUCK1047-BAL-LB5  RESP1024-BAL-LB5  CANUCK1060-BAL-RB4\n",
      "0  ENSG00000290825.1      DDX11L2                    2                   0                 0                   1\n",
      "1  ENSG00000223972.6      DDX11L1                    0                   0                 0                   0\n",
      "2  ENSG00000227232.6       WASH7P                   89                  81                47                 101\n",
      "3  ENSG00000278267.1    MIR6859-1                   23                  12                 7                  14\n",
      "4  ENSG00000243485.5  MIR1302-2HG                    0                   0                 0                   0\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "B DIMENSIONS: rows (genes) = 63187, columns (patients) = 34\n"
     ]
    }
   ],
   "source": [
    "bulk_df = pd.read_csv(BULK_PATH, sep=\"\\t\")\n",
    "\n",
    "print(\"B Matrix (Tissue GEPs) Sample:\\n\")\n",
    "print(bulk_df.iloc[:, :6].head(5))\n",
    "print(\"\\n----------------------------------------------\")\n",
    "print(f\"\\nB DIMENSIONS: rows (genes) = {bulk_df.shape[0]}, columns (patients) = {bulk_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Matrix (Cell GEPs) Sample:\n",
      "\n",
      "                cell_id patient_id  TUBA1A  SPA17  ACTG1  TSTD1  H1-0  NQO1  ATP5IF1  DNPH1  NEDD9  ALDH1A1\n",
      "0  AAACCCACAATACGAA-1_1   BAL-RB-2       0      0      1      1     0     0        0      0      0        0\n",
      "1  AAACGAACACGCTATA-1_1   BAL-RB-2      81      1     64      2     0     1        6      1      0       15\n",
      "2  AACAACCCAAACTCGT-1_1   BAL-RB-2       4      0    106      0     0     0        4      1      0       13\n",
      "3  AACACACCAAATTGGA-1_1   BAL-RB-2       0      0      1      0     0     0        0      0      0        0\n",
      "4  AACAGGGGTCGTACTA-1_1   BAL-RB-2       0      0     16      0     0     0        1      0      2        0\n",
      "\n",
      "----------------------------------------------\n",
      "\n",
      "S DIMENSIONS: rows (patients x cells) = 241924, columns (genes) = 1013\n"
     ]
    }
   ],
   "source": [
    "sc_path = SC_DIR_PATH + \"scRNA_CT1_top200_RawCounts.tsv\"\n",
    "sc_df = pd.read_csv(sc_path, sep=\"\\t\")\n",
    "\n",
    "print(\"S Matrix (Cell GEPs) Sample:\\n\")\n",
    "print(sc_df.iloc[:, :12].head(5))\n",
    "print(\"\\n----------------------------------------------\")\n",
    "print(f\"\\nS DIMENSIONS: rows (patients x cells) = {sc_df.shape[0]}, columns (genes) = {sc_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S Metadata Matrix Sample:\n",
      "\n",
      "                cell_id patient_id  patient_age patient_sex  cell_type_1                cell_type_2                cell_type_3          cell_type_4                      data_source deconv_cluster\n",
      "0  AAACCCACAATACGAA-1_1   BAL-RB-2           32      Female   Epithelial                        NaN                 Epithelial           Epithelial  Post-covid respiratory symptoms     Epithelial\n",
      "1  AAACGAACACGCTATA-1_1   BAL-RB-2           32      Female  Macrophages  Alveolar_Macrophage_CSF1R  Alveolar_Macrophage_CSF1R  Alveolar_macrophage  Post-covid respiratory symptoms    Macrophages\n",
      "2  AACAACCCAAACTCGT-1_1   BAL-RB-2           32      Female  Macrophages           Macrophage_CCL18           Macrophage_CCL18  Alveolar_macrophage  Post-covid respiratory symptoms    Macrophages\n",
      "3  AACACACCAAATTGGA-1_1   BAL-RB-2           32      Female  Macrophages           Macrophage_CCL18           Macrophage_CCL18  Alveolar_macrophage  Post-covid respiratory symptoms    Macrophages\n",
      "4  AACAGGGGTCGTACTA-1_1   BAL-RB-2           32      Female    CD4_Tcell                        NaN                  CD4_Tcell            CD4_Tcell  Post-covid respiratory symptoms      CD4_Tcell\n",
      "\n",
      "----------------------------------------------\n",
      "S Metadata Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 241924 entries, 0 to 241923\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   cell_id         241924 non-null  object\n",
      " 1   patient_id      241924 non-null  object\n",
      " 2   patient_age     241924 non-null  int64 \n",
      " 3   patient_sex     235944 non-null  object\n",
      " 4   cell_type_1     241924 non-null  object\n",
      " 5   cell_type_2     209552 non-null  object\n",
      " 6   cell_type_3     241924 non-null  object\n",
      " 7   cell_type_4     241924 non-null  object\n",
      " 8   data_source     241924 non-null  object\n",
      " 9   deconv_cluster  241924 non-null  object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 18.5+ MB\n",
      "----------------------------------------------\n",
      "\n",
      "S METADATA DIMENSIONS: rows (patients x cells) = 241924, columns (metadata) = 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc_metadata_path = SC_DIR_PATH + \"scRNA_CT1_top200_Metadata.tsv\"\n",
    "sc_metadata_df = pd.read_csv(sc_metadata_path, sep=\"\\t\")\n",
    "\n",
    "print(\"S Metadata Matrix Sample:\\n\")\n",
    "print(sc_metadata_df.head(5))\n",
    "print(\"\\n----------------------------------------------\")\n",
    "print(\"S Metadata Info:\\n\")\n",
    "sc_metadata_df.info()\n",
    "print(\"----------------------------------------------\")\n",
    "print(f\"\\nS METADATA DIMENSIONS: rows (patients x cells) = {sc_metadata_df.shape[0]}, columns (metadata) = {sc_metadata_df.shape[1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Processing\n",
    "Process bulk and single-cell data to generate training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_B(bulk: pd.DataFrame, sc: pd.DataFrame, sc_metadata: pd.DataFrame):\n",
    "    # Filter B to keep only common genes with S\n",
    "    bulk_genes_all = bulk[\"gene_symbol\"].str.strip().str.lower()\n",
    "    common_genes = set(bulk_genes_all).intersection(sc.columns[2:].str.strip().str.lower())\n",
    "    filtered_bulk = bulk[bulk_genes_all.isin(common_genes)].drop_duplicates(subset=\"gene_symbol\", keep=\"first\")\n",
    "    filtered_bulk_vals = filtered_bulk.iloc[:, 2:]  # drop gene_id and gene_symbol cols\n",
    "    \n",
    "    # Normalize and convert to np array\n",
    "    B = np.log1p(filtered_bulk_vals.values.T)\n",
    "    print(f\"B dims (patients x genes): {B.shape}\")\n",
    "\n",
    "    # Assert that patient IDs in S match B\n",
    "    sc_patient_ids = sc_metadata['patient_id'].unique()\n",
    "    bulk_patient_ids = filtered_bulk_vals.columns\n",
    "    if not all(i in bulk_patient_ids for i in sc_patient_ids):\n",
    "        raise ValueError(\"Patient IDs in S do not match B. Check mapping.\")\n",
    "\n",
    "    return B, bulk_patient_ids\n",
    "\n",
    "\n",
    "# def process_S(sc_metadata: pd.DataFrame, patient_ids: np.ndarray, n_aug=15, sample_fraction=0.5):\n",
    "#     \"\"\"\n",
    "#     Derive stratified augmented C matrices from S metadata.\n",
    "\n",
    "#     Args:\n",
    "#         sc_metadata (pd.DataFrame): Single-cell metadata containing patient and cell type information.\n",
    "#         patient_ids (np.ndarray): Array of patient IDs in bulk data.\n",
    "#         n_aug (int): Number of augmentations per patient.\n",
    "#         sample_fraction (float): Fraction of cells to sample for each augmentation (e.g., 0.9 for 90%).\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Augmented C matrices (patients x n_augs x cell types).\n",
    "#         list: Successfully processed patient IDs.\n",
    "#     \"\"\"\n",
    "#     ct_labels = sc_metadata[\"cell_type_1\"].dropna().unique()\n",
    "#     C_augs = []  # Augmentations for all patients\n",
    "#     processed_patients = []  # Successfully processed patient IDs\n",
    "\n",
    "#     for pid in patient_ids:\n",
    "#         print(f\"Augmenting Patient {pid}\")\n",
    "#         patient_cells = sc_metadata[sc_metadata[\"patient_id\"] == pid]\n",
    "#         if patient_cells.empty:\n",
    "#             print(f\"  Skipping (no cells)\")\n",
    "#             continue\n",
    "\n",
    "#         patient_augs = []\n",
    "\n",
    "#         for aug_idx in range(n_aug):\n",
    "#             # Stratify sampling: random sample by cell type\n",
    "#             strat_sample = []\n",
    "#             for ct in ct_labels:\n",
    "#                 ct_cells = patient_cells[patient_cells[\"cell_type_1\"] == ct]\n",
    "#                 if not ct_cells.empty:\n",
    "#                     # Ensure sample size is not larger than available cells\n",
    "#                     n_sample = max(1, min(len(ct_cells), int(len(ct_cells) * sample_fraction)))\n",
    "#                     sampled_cells = ct_cells.sample(n=n_sample, replace=False, random_state=aug_idx)\n",
    "#                     strat_sample.append(sampled_cells)\n",
    "\n",
    "#             # Calculate cell type fractions for this augmentation\n",
    "#             sampled_cells = pd.concat(strat_sample) if strat_sample else pd.DataFrame(columns=[\"cell_type_1\"])\n",
    "#             ct_fractions = sampled_cells[\"cell_type_1\"].value_counts(normalize=True)\n",
    "#             all_ct_fractions = {ct: ct_fractions.get(ct, 0.0) for ct in ct_labels}\n",
    "#             patient_augs.append(list(all_ct_fractions.values()))\n",
    "\n",
    "#         C_augs.append(patient_augs)\n",
    "#         processed_patients.append(pid)\n",
    "\n",
    "#     C_augs = np.array(C_augs)\n",
    "#     # Flatten to 2D so each row is an augmentation for a specific patient\n",
    "#     C_flat = C_augs.reshape(-1, C_augs.shape[2])\n",
    "#     print(f\"C dims ((patients * n_augs) x CTs): {C_flat.shape}\")\n",
    "#     print(f\"Processed patients: {len(processed_patients)}\")\n",
    "\n",
    "#     return C_flat, processed_patients\n",
    "\n",
    "\n",
    "def process_S(sc_metadata: pd.DataFrame, patient_ids: np.ndarray, n_aug=15, sample_fraction=0.9):\n",
    "    \"\"\"\n",
    "    Derive augmented C matrices with random sampling (not stratified).\n",
    "\n",
    "    Args:\n",
    "        sc_metadata (pd.DataFrame): Single-cell metadata containing patient and cell type information.\n",
    "        patient_ids (np.ndarray): Array of patient IDs in bulk data.\n",
    "        n_aug (int): Number of augmentations per patient.\n",
    "        sample_fraction (float): Fraction of cells to sample for each augmentation (e.g., 0.9 for 90%).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Augmented C matrices (patients x n_augs x cell types).\n",
    "        list: Successfully processed patient IDs.\n",
    "    \"\"\"\n",
    "    ct_labels = sc_metadata[\"cell_type_1\"].dropna().unique()\n",
    "    C_augs = []  # Augmentations for all patients\n",
    "    processed_patients = []  # Successfully processed patient IDs\n",
    "\n",
    "    for pid in patient_ids:\n",
    "        print(f\"Augmenting Patient {pid}\")\n",
    "        patient_cells = sc_metadata[sc_metadata[\"patient_id\"] == pid]\n",
    "        if patient_cells.empty:\n",
    "            print(f\"  Skipping (no cells)\")\n",
    "            continue\n",
    "\n",
    "        patient_augs = []\n",
    "\n",
    "        for _ in range(n_aug):\n",
    "            # Randomly sample a subset of all cells\n",
    "            n_sample = max(1, int(len(patient_cells) * sample_fraction))\n",
    "            sampled_cells = patient_cells.sample(n=n_sample, replace=False, random_state=None)\n",
    "\n",
    "            # Calculate cell type fractions for this augmentation\n",
    "            ct_fractions = sampled_cells[\"cell_type_1\"].value_counts(normalize=True)\n",
    "            all_ct_fractions = {ct: ct_fractions.get(ct, 0.0) for ct in ct_labels}\n",
    "            patient_augs.append(list(all_ct_fractions.values()))\n",
    "\n",
    "        C_augs.append(patient_augs)\n",
    "        processed_patients.append(pid)\n",
    "\n",
    "    C_augs = np.array(C_augs)\n",
    "    # Flatten to 2D so each row is an augmentation for a specific patient\n",
    "    C_flat = C_augs.reshape(-1, C_augs.shape[2])\n",
    "    print(f\"C dims ((patients * n_augs) x CTs): {C_flat.shape}\")\n",
    "    print(f\"Processed patients: {len(processed_patients)}\")\n",
    "\n",
    "    return C_flat, processed_patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B dims (patients x genes): (32, 1009)\n",
      "Augmenting Patient CANUCK1057-BAL-LB3B\n",
      "Augmenting Patient CANUCK1047-BAL-LB5\n",
      "Augmenting Patient RESP1024-BAL-LB5\n",
      "Augmenting Patient CANUCK1060-BAL-RB4\n",
      "  Skipping (no cells)\n",
      "Augmenting Patient RESPONSE1030-BAL-RB5\n",
      "Augmenting Patient CANUCK1012-BAL\n",
      "Augmenting Patient RESPONSE1094-BAL-LB5\n",
      "Augmenting Patient VAPE1007-BAL-LB5\n",
      "Augmenting Patient RESP1022-BAL-LB1\n",
      "Augmenting Patient BAL-RB-2\n",
      "Augmenting Patient VAPE1009-BAL-RB5\n",
      "Augmenting Patient RESP1023-BAL-LB5\n",
      "Augmenting Patient RESP1019-BAL-RB9\n",
      "Augmenting Patient RESP1038-BAL-RB5\n",
      "Augmenting Patient RESP1001-BAL-LB\n",
      "Augmenting Patient Response1014-BAL\n",
      "Augmenting Patient RESP1036-BAL-LB5\n",
      "Augmenting Patient VAPE1010-BAL-RB5\n",
      "Augmenting Patient RESPONSE1040-BAL-LB5\n",
      "Augmenting Patient CANUCK1043-BAL-RB5\n",
      "Augmenting Patient CANUCK1039-BAL-RB6\n",
      "  Skipping (no cells)\n",
      "Augmenting Patient CANUCK1031-BAL-LB5\n",
      "Augmenting Patient RESP1020-BAL-LB5\n",
      "Augmenting Patient CANUCK1035-BAL-RB5\n",
      "Augmenting Patient RESPONSE1076-BAL-LB5\n",
      "Augmenting Patient VAPE1013-BAL-LB5\n",
      "Augmenting Patient CANUCK1091-BAL-LB5\n",
      "Augmenting Patient LB1-2-BAL\n",
      "Augmenting Patient RESPONSE1092-BAL-RB5\n",
      "Augmenting Patient CANUCK1020-LB5\n",
      "Augmenting Patient RESP1045-BAL-LB5\n",
      "Augmenting Patient CANUCK1096-BAL-RB5\n",
      "C dims ((patients * n_augs) x CTs): (900, 11)\n",
      "Processed patients: 30\n",
      "Filtered B dims (patients x genes): (30, 1009)\n",
      "Flattened B dims ((patients * n_augs) x genes): (900, 1009)\n"
     ]
    }
   ],
   "source": [
    "n_aug = 30\n",
    "B, patient_ids = process_B(bulk_df, sc_df, sc_metadata_df)\n",
    "C_flat, processed_patient_ids = process_S(sc_metadata_df, patient_ids, n_aug)\n",
    "\n",
    "B_filtered = B[np.isin(patient_ids, processed_patient_ids)]\n",
    "print(f\"Filtered B dims (patients x genes): {B_filtered.shape}\")\n",
    "\n",
    "B_aug = np.repeat(B_filtered, n_aug, axis=0)  # Repeat B for each augmentation\n",
    "print(f\"Flattened B dims ((patients * n_augs) x genes): {B_aug.shape}\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(B_aug, C_flat, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(Y_train, dtype=torch.float32),\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32),\n",
    "    torch.tensor(Y_test, dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C sample:\n",
      "[[0.0708 0.8299 0.0153 0.0289 0.0075 0.0063 0.0019 0.0078 0.0048 0.0002 0.0266]\n",
      " [0.068  0.8331 0.0149 0.0274 0.0075 0.006  0.002  0.008  0.0048 0.0002 0.0281]\n",
      " [0.0702 0.8305 0.0158 0.0285 0.0073 0.0061 0.0019 0.0067 0.0048 0.0002 0.0279]\n",
      " [0.0691 0.8323 0.0151 0.0279 0.0071 0.0058 0.002  0.0078 0.005  0.0002 0.0276]\n",
      " [0.0702 0.8301 0.0155 0.0285 0.0073 0.0056 0.0019 0.0084 0.005  0.0002 0.0274]\n",
      " [0.0706 0.8318 0.0155 0.0278 0.0063 0.0063 0.002  0.0076 0.005  0.0002 0.0268]\n",
      " [0.071  0.8282 0.0166 0.0287 0.0078 0.0054 0.0017 0.0076 0.0047 0.0002 0.0281]\n",
      " [0.0704 0.8307 0.016  0.0279 0.0075 0.0061 0.002  0.0073 0.005  0.0002 0.0268]\n",
      " [0.0693 0.8312 0.0149 0.0292 0.0076 0.0061 0.0017 0.0082 0.0043 0.0002 0.0272]\n",
      " [0.0708 0.829  0.0156 0.0287 0.0075 0.0061 0.0019 0.0076 0.0054 0.0002 0.0272]]\n"
     ]
    }
   ],
   "source": [
    "print(\"C sample:\")\n",
    "print(C_flat[10:20, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "Define model architecture/parameters, run training, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2dRNA(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model2dRNA, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(500),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(500, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model: Model2dRNA, train_set: TensorDataset, test_set: TensorDataset, epochs: int, batch_size: int):\n",
    "    model.to(DEVICE)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in test_loader:\n",
    "                X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE)\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss += criterion(val_outputs, y_val).item()\n",
    "        print(f\"Epoch {e+1}/{epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "def save_model(model: Model2dRNA, X_test, Y_test):\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    dtnum = str(datetime.datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "    model_dir = os.path.join(\"output\", \"2dRNA\", dtnum)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(model_dir, \"model.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    # Save predictions and true fractions\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test.to(DEVICE)).cpu().numpy()\n",
    "    preds_file = os.path.join(model_dir, \"pred_fractions.csv\")\n",
    "    true_fractions_file = os.path.join(model_dir, \"true_fractions.csv\")\n",
    "    np.savetxt(preds_file, predictions, delimiter=\",\")\n",
    "    np.savetxt(true_fractions_file, Y_test.numpy(), delimiter=\",\")\n",
    "    print(f\"Saved predictions to {preds_file}\")\n",
    "    print(f\"Saved true fractions to {true_fractions_file}\")\n",
    "\n",
    "\n",
    "def eval_model(model: Model2dRNA, X_test, Y_test):\n",
    "    print(\"\\nEvaluating model on Y_test:\")\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_test = torch.tensor(Y_test, dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Y_pred = model(X_test.to(DEVICE)).cpu()\n",
    "\n",
    "    target_min = Y_test.min()\n",
    "    target_max = Y_test.max()\n",
    "    target_mean = Y_test.mean()\n",
    "\n",
    "    mae = torch.mean(torch.abs(Y_pred - Y_test)).item()\n",
    "    rmse = torch.sqrt(torch.mean((Y_pred - Y_test) ** 2)).item()\n",
    "    cosine = torch.nn.functional.cosine_similarity(Y_pred, Y_test, dim=1).mean().item()\n",
    "\n",
    "    mae_pct_range = (mae / (target_max - target_min)) * 100\n",
    "    mae_pct_mean = (mae / target_mean) * 100\n",
    "    rmse_pct_range = (rmse / (target_max - target_min)) * 100\n",
    "    rmse_pct_mean = (rmse / target_mean) * 100\n",
    "\n",
    "    print(f\" - Target value range: [{target_min:.4f}, {target_max:.4f}]\")\n",
    "    print(f\" - Target value average: {target_mean:.4f}\")\n",
    "    print(f\" - MAE: {mae:.4f}\")\n",
    "    print(f\" - MAE as percentage of range: {mae_pct_range:.2f}%\")\n",
    "    print(f\" - MAE as percentage of average: {mae_pct_mean:.2f}%\")\n",
    "    print(f\" - RMSE: {rmse:.4f}\")\n",
    "    print(f\" - RMSE as percentage of range: {rmse_pct_range:.2f}%\")\n",
    "    print(f\" - RMSE as percentage of average: {rmse_pct_mean:.2f}%\")\n",
    "    print(f\" - Cosine similarity: {cosine:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/150, Loss: 4.3714, Val Loss: 4.4960\n",
      "Epoch 2/150, Loss: 1.5722, Val Loss: 0.4129\n",
      "Epoch 3/150, Loss: 0.9700, Val Loss: 0.1413\n",
      "Epoch 4/150, Loss: 0.5277, Val Loss: 0.0542\n",
      "Epoch 5/150, Loss: 0.2799, Val Loss: 0.0141\n",
      "Epoch 6/150, Loss: 0.1602, Val Loss: 0.0063\n",
      "Epoch 7/150, Loss: 0.1303, Val Loss: 0.0070\n",
      "Epoch 8/150, Loss: 0.1045, Val Loss: 0.0059\n",
      "Epoch 9/150, Loss: 0.1007, Val Loss: 0.0051\n",
      "Epoch 10/150, Loss: 0.0876, Val Loss: 0.0035\n",
      "Epoch 11/150, Loss: 0.0789, Val Loss: 0.0046\n",
      "Epoch 12/150, Loss: 0.0702, Val Loss: 0.0031\n",
      "Epoch 13/150, Loss: 0.0637, Val Loss: 0.0035\n",
      "Epoch 14/150, Loss: 0.0598, Val Loss: 0.0024\n",
      "Epoch 15/150, Loss: 0.0594, Val Loss: 0.0031\n",
      "Epoch 16/150, Loss: 0.0531, Val Loss: 0.0038\n",
      "Epoch 17/150, Loss: 0.0484, Val Loss: 0.0019\n",
      "Epoch 18/150, Loss: 0.0415, Val Loss: 0.0018\n",
      "Epoch 19/150, Loss: 0.0392, Val Loss: 0.0020\n",
      "Epoch 20/150, Loss: 0.0408, Val Loss: 0.0022\n",
      "Epoch 21/150, Loss: 0.0378, Val Loss: 0.0023\n",
      "Epoch 22/150, Loss: 0.0370, Val Loss: 0.0016\n",
      "Epoch 23/150, Loss: 0.0320, Val Loss: 0.0018\n",
      "Epoch 24/150, Loss: 0.0310, Val Loss: 0.0012\n",
      "Epoch 25/150, Loss: 0.0287, Val Loss: 0.0013\n",
      "Epoch 26/150, Loss: 0.0269, Val Loss: 0.0013\n",
      "Epoch 27/150, Loss: 0.0276, Val Loss: 0.0010\n",
      "Epoch 28/150, Loss: 0.0293, Val Loss: 0.0010\n",
      "Epoch 29/150, Loss: 0.0258, Val Loss: 0.0013\n",
      "Epoch 30/150, Loss: 0.0244, Val Loss: 0.0019\n",
      "Epoch 31/150, Loss: 0.0256, Val Loss: 0.0025\n",
      "Epoch 32/150, Loss: 0.0226, Val Loss: 0.0010\n",
      "Epoch 33/150, Loss: 0.0209, Val Loss: 0.0015\n",
      "Epoch 34/150, Loss: 0.0203, Val Loss: 0.0014\n",
      "Epoch 35/150, Loss: 0.0203, Val Loss: 0.0009\n",
      "Epoch 36/150, Loss: 0.0186, Val Loss: 0.0018\n",
      "Epoch 37/150, Loss: 0.0208, Val Loss: 0.0014\n",
      "Epoch 38/150, Loss: 0.0200, Val Loss: 0.0092\n",
      "Epoch 39/150, Loss: 0.0193, Val Loss: 0.0011\n",
      "Epoch 40/150, Loss: 0.0161, Val Loss: 0.0007\n",
      "Epoch 41/150, Loss: 0.0204, Val Loss: 0.0011\n",
      "Epoch 42/150, Loss: 0.0184, Val Loss: 0.0009\n",
      "Epoch 43/150, Loss: 0.0168, Val Loss: 0.0012\n",
      "Epoch 44/150, Loss: 0.0171, Val Loss: 0.0012\n",
      "Epoch 45/150, Loss: 0.0163, Val Loss: 0.0008\n",
      "Epoch 46/150, Loss: 0.0176, Val Loss: 0.0009\n",
      "Epoch 47/150, Loss: 0.0170, Val Loss: 0.0005\n",
      "Epoch 48/150, Loss: 0.0175, Val Loss: 0.0019\n",
      "Epoch 49/150, Loss: 0.0159, Val Loss: 0.0013\n",
      "Epoch 50/150, Loss: 0.0158, Val Loss: 0.0007\n",
      "Epoch 51/150, Loss: 0.0144, Val Loss: 0.0017\n",
      "Epoch 52/150, Loss: 0.0151, Val Loss: 0.0007\n",
      "Epoch 53/150, Loss: 0.0163, Val Loss: 0.0008\n",
      "Epoch 54/150, Loss: 0.0138, Val Loss: 0.0010\n",
      "Epoch 55/150, Loss: 0.0128, Val Loss: 0.0013\n",
      "Epoch 56/150, Loss: 0.0133, Val Loss: 0.0010\n",
      "Epoch 57/150, Loss: 0.0140, Val Loss: 0.0016\n",
      "Epoch 58/150, Loss: 0.0128, Val Loss: 0.0013\n",
      "Epoch 59/150, Loss: 0.0135, Val Loss: 0.0008\n",
      "Epoch 60/150, Loss: 0.0133, Val Loss: 0.0006\n",
      "Epoch 61/150, Loss: 0.0123, Val Loss: 0.0009\n",
      "Epoch 62/150, Loss: 0.0114, Val Loss: 0.0012\n",
      "Epoch 63/150, Loss: 0.0114, Val Loss: 0.0006\n",
      "Epoch 64/150, Loss: 0.0121, Val Loss: 0.0008\n",
      "Epoch 65/150, Loss: 0.0122, Val Loss: 0.0007\n",
      "Epoch 66/150, Loss: 0.0115, Val Loss: 0.0007\n",
      "Epoch 67/150, Loss: 0.0108, Val Loss: 0.0008\n",
      "Epoch 68/150, Loss: 0.0106, Val Loss: 0.0007\n",
      "Epoch 69/150, Loss: 0.0113, Val Loss: 0.0009\n",
      "Epoch 70/150, Loss: 0.0118, Val Loss: 0.0008\n",
      "Epoch 71/150, Loss: 0.0106, Val Loss: 0.0006\n",
      "Epoch 72/150, Loss: 0.0093, Val Loss: 0.0004\n",
      "Epoch 73/150, Loss: 0.0106, Val Loss: 0.0006\n",
      "Epoch 74/150, Loss: 0.0109, Val Loss: 0.0005\n",
      "Epoch 75/150, Loss: 0.0094, Val Loss: 0.0007\n",
      "Epoch 76/150, Loss: 0.0098, Val Loss: 0.0008\n",
      "Epoch 77/150, Loss: 0.0101, Val Loss: 0.0010\n",
      "Epoch 78/150, Loss: 0.0092, Val Loss: 0.0006\n",
      "Epoch 79/150, Loss: 0.0099, Val Loss: 0.0007\n",
      "Epoch 80/150, Loss: 0.0120, Val Loss: 0.0006\n",
      "Epoch 81/150, Loss: 0.0075, Val Loss: 0.0004\n",
      "Epoch 82/150, Loss: 0.0092, Val Loss: 0.0007\n",
      "Epoch 83/150, Loss: 0.0079, Val Loss: 0.0007\n",
      "Epoch 84/150, Loss: 0.0088, Val Loss: 0.0008\n",
      "Epoch 85/150, Loss: 0.0108, Val Loss: 0.0007\n",
      "Epoch 86/150, Loss: 0.0100, Val Loss: 0.0007\n",
      "Epoch 87/150, Loss: 0.0111, Val Loss: 0.0004\n",
      "Epoch 88/150, Loss: 0.0079, Val Loss: 0.0007\n",
      "Epoch 89/150, Loss: 0.0086, Val Loss: 0.0012\n",
      "Epoch 90/150, Loss: 0.0102, Val Loss: 0.0005\n",
      "Epoch 91/150, Loss: 0.0092, Val Loss: 0.0006\n",
      "Epoch 92/150, Loss: 0.0091, Val Loss: 0.0010\n",
      "Epoch 93/150, Loss: 0.0095, Val Loss: 0.0007\n",
      "Epoch 94/150, Loss: 0.0091, Val Loss: 0.0004\n",
      "Epoch 95/150, Loss: 0.0083, Val Loss: 0.0009\n",
      "Epoch 96/150, Loss: 0.0086, Val Loss: 0.0006\n",
      "Epoch 97/150, Loss: 0.0091, Val Loss: 0.0014\n",
      "Epoch 98/150, Loss: 0.0070, Val Loss: 0.0006\n",
      "Epoch 99/150, Loss: 0.0075, Val Loss: 0.0009\n",
      "Epoch 100/150, Loss: 0.0074, Val Loss: 0.0004\n",
      "Epoch 101/150, Loss: 0.0067, Val Loss: 0.0006\n",
      "Epoch 102/150, Loss: 0.0086, Val Loss: 0.0005\n",
      "Epoch 103/150, Loss: 0.0091, Val Loss: 0.0009\n",
      "Epoch 104/150, Loss: 0.0075, Val Loss: 0.0003\n",
      "Epoch 105/150, Loss: 0.0074, Val Loss: 0.0006\n",
      "Epoch 106/150, Loss: 0.0075, Val Loss: 0.0005\n",
      "Epoch 107/150, Loss: 0.0103, Val Loss: 0.0011\n",
      "Epoch 108/150, Loss: 0.0092, Val Loss: 0.0005\n",
      "Epoch 109/150, Loss: 0.0067, Val Loss: 0.0004\n",
      "Epoch 110/150, Loss: 0.0061, Val Loss: 0.0005\n",
      "Epoch 111/150, Loss: 0.0076, Val Loss: 0.0007\n",
      "Epoch 112/150, Loss: 0.0078, Val Loss: 0.0005\n",
      "Epoch 113/150, Loss: 0.0080, Val Loss: 0.0015\n",
      "Epoch 114/150, Loss: 0.0081, Val Loss: 0.0006\n",
      "Epoch 115/150, Loss: 0.0068, Val Loss: 0.0006\n",
      "Epoch 116/150, Loss: 0.0065, Val Loss: 0.0005\n",
      "Epoch 117/150, Loss: 0.0070, Val Loss: 0.0008\n",
      "Epoch 118/150, Loss: 0.0061, Val Loss: 0.0007\n",
      "Epoch 119/150, Loss: 0.0079, Val Loss: 0.0003\n",
      "Epoch 120/150, Loss: 0.0066, Val Loss: 0.0008\n",
      "Epoch 121/150, Loss: 0.0071, Val Loss: 0.0008\n",
      "Epoch 122/150, Loss: 0.0074, Val Loss: 0.0007\n",
      "Epoch 123/150, Loss: 0.0070, Val Loss: 0.0009\n",
      "Epoch 124/150, Loss: 0.0094, Val Loss: 0.0012\n",
      "Epoch 125/150, Loss: 0.0086, Val Loss: 0.0005\n",
      "Epoch 126/150, Loss: 0.0071, Val Loss: 0.0006\n",
      "Epoch 127/150, Loss: 0.0063, Val Loss: 0.0004\n",
      "Epoch 128/150, Loss: 0.0078, Val Loss: 0.0007\n",
      "Epoch 129/150, Loss: 0.0081, Val Loss: 0.0008\n",
      "Epoch 130/150, Loss: 0.0075, Val Loss: 0.0009\n",
      "Epoch 131/150, Loss: 0.0084, Val Loss: 0.0007\n",
      "Epoch 132/150, Loss: 0.0076, Val Loss: 0.0009\n",
      "Epoch 133/150, Loss: 0.0066, Val Loss: 0.0003\n",
      "Epoch 134/150, Loss: 0.0051, Val Loss: 0.0004\n",
      "Epoch 135/150, Loss: 0.0067, Val Loss: 0.0007\n",
      "Epoch 136/150, Loss: 0.0062, Val Loss: 0.0006\n",
      "Epoch 137/150, Loss: 0.0065, Val Loss: 0.0006\n",
      "Epoch 138/150, Loss: 0.0067, Val Loss: 0.0003\n",
      "Epoch 139/150, Loss: 0.0059, Val Loss: 0.0004\n",
      "Epoch 140/150, Loss: 0.0059, Val Loss: 0.0004\n",
      "Epoch 141/150, Loss: 0.0067, Val Loss: 0.0006\n",
      "Epoch 142/150, Loss: 0.0056, Val Loss: 0.0002\n",
      "Epoch 143/150, Loss: 0.0046, Val Loss: 0.0002\n",
      "Epoch 144/150, Loss: 0.0051, Val Loss: 0.0005\n",
      "Epoch 145/150, Loss: 0.0058, Val Loss: 0.0007\n",
      "Epoch 146/150, Loss: 0.0062, Val Loss: 0.0003\n",
      "Epoch 147/150, Loss: 0.0069, Val Loss: 0.0007\n",
      "Epoch 148/150, Loss: 0.0056, Val Loss: 0.0005\n",
      "Epoch 149/150, Loss: 0.0065, Val Loss: 0.0009\n",
      "Epoch 150/150, Loss: 0.0063, Val Loss: 0.0005\n",
      "Training complete!\n",
      "Saved model to output/2dRNA/20250108_2100/model.pth\n",
      "Saved predictions to output/2dRNA/20250108_2100/pred_fractions.csv\n",
      "Saved true fractions to output/2dRNA/20250108_2100/true_fractions.csv\n",
      "\n",
      "Evaluating model on Y_test:\n",
      " - Target value range: [0.0000, 0.9914]\n",
      " - Target value average: 0.0909\n",
      " - MAE: 0.0052\n",
      " - MAE as percentage of range: 0.52%\n",
      " - MAE as percentage of average: 5.70%\n",
      " - RMSE: 0.0095\n",
      " - RMSE as percentage of range: 0.96%\n",
      " - RMSE as percentage of average: 10.43%\n",
      " - Cosine similarity: 0.9993\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "output_dim = Y_train.shape[1]\n",
    "model = Model2dRNA(input_dim, output_dim)\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "saved_model_path = None # \"output/2dRNA/20241229_1515/model.pth\"\n",
    "if saved_model_path and os.path.exists(saved_model_path):\n",
    "    model.load_state_dict(torch.load(saved_model_path))\n",
    "    print(f\"Loaded model from {saved_model_path}\")\n",
    "else:\n",
    "    print(\"Training model...\")\n",
    "    train_model(model, train_dataset, test_dataset, epochs, batch_size)\n",
    "    print(\"Training complete!\")\n",
    "    save_model(model, X_test, Y_test)\n",
    "\n",
    "eval_model(model, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "based",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
